{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7f4f69",
   "metadata": {},
   "source": [
    "# Compute ML performance metrics by group (sex) and run Google's Fairness Indicators using TFMA\n",
    "This Colab-ready notebook explains why we compute metrics by protected groups and walks through running Google's Fairness Indicators on the Adult dataset. Run each cell in order. If you opened this in VS Code, you can also run it locally if TFMA installs on your platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup: Install and import libraries\n",
    "%%bash\n",
    "pip install -q pandas numpy scikit-learn matplotlib seaborn\n",
    "pip install -q \"tensorflow>=2.16.0\" tensorflow-model-analysis fairness-indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be111823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json, os, zipfile\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "from fairness_indicators.tfx import util as fi_util\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "ARTIFACTS = Path('/content/outputs' if Path('/content').exists() else 'outputs')\n",
    "ARTIFACTS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load and prepare dataset (features, label, sensitive attribute: sex)\n",
    "adult = fetch_openml('adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "df['income_binary'] = (df['class'] == '>50K').astype(int)\n",
    "df = df.drop(columns=['class'])\n",
    "sensitive = df['sex'].copy()  # keep raw sex for slicing\n",
    "df = df.drop(columns=['sex'])\n",
    "y = df['income_binary']\n",
    "X = df.drop(columns=['income_binary'])\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=['object','category']).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ('num', StandardScaler(with_mean=False), num_cols)\n",
    "])\n",
    "\n",
    "# 3) Split data and train baseline classifier\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = Pipeline(steps=[('prep', preprocess), ('lr', LogisticRegression(max_iter=1000))])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save for reference\n",
    "import joblib\n",
    "joblib.dump(clf, ARTIFACTS / 'logreg_adult.joblib')\n",
    "print('Model trained and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Evaluate aggregate performance metrics\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "metrics_agg = {\n",
    "    'accuracy': accuracy_score(y_test, preds),\n",
    "    'precision': precision_score(y_test, preds),\n",
    "    'recall': recall_score(y_test, preds),\n",
    "    'f1': f1_score(y_test, preds),\n",
    "    'roc_auc': roc_auc_score(y_test, probs),\n",
    "    'pr_auc': average_precision_score(y_test, probs),\n",
    "}\n",
    "print(metrics_agg)\n",
    "json.dump(metrics_agg, open(ARTIFACTS / 'metrics_aggregate.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e83379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Compute metrics by group (sex) and fairness gaps\n",
    "def rates(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    pos = y_pred == 1\n",
    "    neg = y_pred == 0\n",
    "    tp = np.sum((y_true==1)&pos)\n",
    "    fp = np.sum((y_true==0)&pos)\n",
    "    fn = np.sum((y_true==1)&neg)\n",
    "    tn = np.sum((y_true==0)&neg)\n",
    "    selection = float(np.mean(pos)) if len(y_true) else 0.0\n",
    "    tpr = tp / (tp + fn + 1e-9)\n",
    "    fpr = fp / (fp + tn + 1e-9)\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec = tpr\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "    return selection,tpr,fpr,prec,rec,f1\n",
    "\n",
    "df_groups = []\n",
    "for g in sorted(sens_test.unique()):\n",
    "    mask = sens_test==g\n",
    "    sel,tpr,fpr,prec,rec,f1 = rates(y_test[mask], preds[mask])\n",
    "    df_groups.append({'sex': g, 'selection_rate': sel, 'tpr': tpr, 'fpr': fpr, 'precision':prec, 'recall':rec, 'f1':f1})\n",
    "import pandas as pd\n",
    "by_group = pd.DataFrame(df_groups)\n",
    "by_group.to_csv(ARTIFACTS / 'metrics_by_group.csv', index=False)\n",
    "print(by_group)\n",
    "\n",
    "# Choose reference group as 'Male' if present\n",
    "if 'Male' in by_group['sex'].values:\n",
    "    ref = by_group[by_group['sex']=='Male'].iloc[0]\n",
    "else:\n",
    "    ref = by_group.iloc[0]\n",
    "other = by_group[by_group['sex']!=ref['sex']].iloc[0] if len(by_group)>1 else ref\n",
    "\n",
    "delta_dp = other['selection_rate'] - ref['selection_rate']\n",
    "ratio_sr = (other['selection_rate'] / (ref['selection_rate']+1e-9))\n",
    "delta_tpr = other['tpr'] - ref['tpr']\n",
    "delta_fpr = other['fpr'] - ref['fpr']\n",
    "fairness = {'delta_demographic_parity': float(delta_dp), 'selection_rate_ratio': float(ratio_sr), 'delta_tpr': float(delta_tpr), 'delta_fpr': float(delta_fpr)}\n",
    "print(fairness)\n",
    "json.dump(fairness, open(ARTIFACTS / 'fairness_gaps.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f32b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Visualize group metrics and gaps\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=by_group, x='sex', y='selection_rate')\n",
    "plt.title('Selection rate by sex')\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS / 'plot_selection_rate_by_sex.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 7) Threshold sweep and ROC/PR by group (quick sketch)\n",
    "ths = np.linspace(0,1,101)\n",
    "roc = {}\n",
    "for g in sorted(sens_test.unique()):\n",
    "    mask = sens_test==g\n",
    "    ys = y_test[mask].values\n",
    "    gr = []\n",
    "    for t in ths:\n",
    "        pp = (probs[mask] >= t).astype(int)\n",
    "        sel,tpr,fpr,_,_,_ = rates(ys, pp)\n",
    "        gr.append((t,tpr,fpr,sel))\n",
    "    roc[g] = pd.DataFrame(gr, columns=['thr','tpr','fpr','sel'])\n",
    "# Save a CSV snapshot\n",
    "with pd.ExcelWriter(ARTIFACTS / 'threshold_sweep.xlsx') as xl:\n",
    "    for g,dfg in roc.items():\n",
    "        dfg.to_excel(xl, sheet_name=str(g)[:30], index=False)\n",
    "print('Saved threshold_sweep.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Export TFMA artifacts and 9) Run Google Fairness Indicators\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "# Build a DataFrame with raw features (including sex), labels and predictions\n",
    "test_raw = X_test.copy()\n",
    "test_raw['sex'] = sens_test.values\n",
    "test_raw['label'] = y_test.values\n",
    "test_raw['pred'] = probs\n",
    "\n",
    "# Define EvalConfig with slicing on sex\n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='label', prediction_key='pred')],\n",
    "    slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['sex'])],\n",
    "    metrics_specs=[tfma.MetricsSpec(metrics=[\n",
    "        tfma.MetricConfig(class_name='ExampleCount'),\n",
    "        tfma.MetricConfig(class_name='AUC'),\n",
    "        tfma.MetricConfig(class_name='Accuracy'),\n",
    "        tfma.MetricConfig(class_name='Precision'),\n",
    "        tfma.MetricConfig(class_name='Recall'),\n",
    "        tfma.MetricConfig(class_name='TruePositives'),\n",
    "        tfma.MetricConfig(class_name='FalsePositives'),\n",
    "        tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "        tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "        tfma.MetricConfig(class_name='Calibration'),\n",
    "    ])]\n",
    " )\n",
    "\n",
    "# Run evaluation from a Pandas DataFrame\n",
    "eval_result = tfma.evaluate(\n",
    "    extracts=test_raw,\n",
    "    eval_config=eval_config,\n",
    "    output_path=str(ARTIFACTS / 'tfma_eval'),\n",
    "    evaluation_options=tfma.options.EvaluationOptions(\n",
    "        slicing_evaluation=True, numpy_array_override=True))\n",
    "\n",
    "# Display Fairness Indicators widget\n",
    "from fairness_indicators.widget import fairness_indicators_v2\n",
    "fairness_indicators_v2.render_fairness_indicators(eval_result)\n",
    "print('TFMA evaluation complete. Artifacts at', ARTIFACTS / 'tfma_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Configure fairness thresholds and automated checks\n",
    "thresholds = {\n",
    "    'min_selection_rate_ratio': 0.8,  # 80% rule\n",
    "    'max_delta_tpr': 0.1,\n",
    "    'max_delta_fpr': 0.1,\n",
    "}\n",
    "violations = {}\n",
    "with open(ARTIFACTS / 'fairness_gaps.json') as f:\n",
    "    fg = json.load(f)\n",
    "violations['selection_rate_ratio'] = float(fg['selection_rate_ratio'] < thresholds['min_selection_rate_ratio'])\n",
    "violations['delta_tpr'] = float(abs(fg['delta_tpr']) > thresholds['max_delta_tpr'])\n",
    "violations['delta_fpr'] = float(abs(fg['delta_fpr']) > thresholds['max_delta_fpr'])\n",
    "json.dump(violations, open(ARTIFACTS / 'fairness_violations.json','w'))\n",
    "print('Fairness violations:', violations)\n",
    "if any(v>0 for v in violations.values()):\n",
    "    print('One or more fairness checks failed (non-blocking in notebook).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Simple mitigation: group-specific thresholds and re-evaluate\n",
    "mitigated = {}\n",
    "for g in sorted(sens_test.unique()):\n",
    "    mask = sens_test==g\n",
    "    # choose threshold to match selection rate to closest of groups (simple heuristic)\n",
    "    target_sr = by_group['selection_rate'].mean()\n",
    "    th_best, diff_best = 0.5, 1e9\n",
    "    for t in np.linspace(0,1,101):\n",
    "        sr = float(np.mean((probs[mask]>=t).astype(int)))\n",
    "        d = abs(sr - target_sr)\n",
    "        if d < diff_best: th_best, diff_best = t, d\n",
    "    mitigated[g] = th_best\n",
    "mitigated\n",
    "mit_preds = np.zeros_like(preds)\n",
    "for g,th in mitigated.items():\n",
    "    mask = sens_test==g\n",
    "    mit_preds[mask] = (probs[mask]>=th).astype(int)\n",
    "mit_df = []\n",
    "for g in sorted(sens_test.unique()):\n",
    "    mask = sens_test==g\n",
    "    sel,tpr,fpr,prec,rec,f1 = rates(y_test[mask], mit_preds[mask])\n",
    "    mit_df.append({'sex':g,'selection_rate':sel,'tpr':tpr,'fpr':fpr,'precision':prec,'recall':rec,'f1':f1,'thr':mitigated[g]})\n",
    "mit_df = pd.DataFrame(mit_df)\n",
    "mit_df.to_csv(ARTIFACTS / 'metrics_by_group_mitigated.csv', index=False)\n",
    "print(mit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Unit tests for metric calculations (optional)\n",
    "%%bash\n",
    "python - <<'PY'\n",
    "import numpy as np\n",
    "def rates(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    pos = y_pred == 1\n",
    "    neg = y_pred == 0\n",
    "    tp = np.sum((y_true==1)&pos)\n",
    "    fp = np.sum((y_true==0)&pos)\n",
    "    fn = np.sum((y_true==1)&neg)\n",
    "    tn = np.sum((y_true==0)&neg)\n",
    "    selection = float(np.mean(pos)) if len(y_true) else 0.0\n",
    "    tpr = tp / (tp + fn + 1e-9)\n",
    "    fpr = fp / (fp + tn + 1e-9)\n",
    "    return selection,tpr,fpr\n",
    "# Happy path\n",
    "sel,tpr,fpr = rates([1,0,1,0],[1,0,0,1])\n",
    "assert abs(sel-0.5) < 1e-6 and abs(tpr-0.5)<1e-6 and abs(fpr-0.5)<1e-6\n",
    "# Edge: no positives\n",
    "sel,tpr,fpr = rates([0,0,0],[0,1,0])\n",
    "assert 0.0<=tpr<=1.0 and 0.0<=fpr<=1.0\n",
    "print('Metric tests passed')\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Save artifacts and lightweight report (+ download as zip in Colab)\n",
    "summary = {\n",
    "    'aggregate': json.load(open(ARTIFACTS / 'metrics_aggregate.json')),\n",
    "    'fairness_gaps': json.load(open(ARTIFACTS / 'fairness_gaps.json')),\n",
    "}\n",
    "json.dump(summary, open(ARTIFACTS / 'summary_google.json','w'))\n",
    "print('Wrote', ARTIFACTS / 'summary_google.json')\n",
    "\n",
    "if str(ARTIFACTS).startswith('/content'):\n",
    "    zpath = '/content/outputs_zip.zip'\n",
    "    with zipfile.ZipFile(zpath, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in ARTIFACTS.glob('**/*'):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=p.name)\n",
    "    print('Download artifacts:', zpath)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
